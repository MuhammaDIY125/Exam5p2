# ML Модель для Kaggle: Минимизация MAE
### Описание проекта
Здесь я собрал модель машинного обучения для соревнования на **Kaggle**, где основной целью было добиться минимума по метрике MAE.

### Этапы работы
1. Создание дополнительных признаков
- Сначала я добавил новые признаки чтобы увеличить информативность данных.

2. Отбор признаков
- Посмотрев на корреляцию я сразу же выбросил признаки с очень высоким между собой и практически нулевым с другими. Затем я сделал стэк моделей **Random Forest**, **Huber** и **Ridge** (в качестве основной выбрал именно его) и при помощи него отбирал остальные. Было сложно выбирать т.к. несколько признаков имели слишком высокую корреляцию.

3. Оптимизация гиперпараметров (HPO) с помощью **Optuna** и большое разачерование
- Для подбора гиперпараметров я использовал **Optuna**, что позволило более точно настроить модели и увеличить их производительность. Но даже так я не смог получить приемлимые результаты. Затем я попробовал сделать простую модель Random Forest который в одиночку показал результаты лучше, чем весь стэк. Именно его я в следствии начал улучшать и достраивать.

4. Обучение на **Google Colab**
- Чтобы ускорить обучение, запускал модель в **Google Colab**, что позволило быстрее обрабатывать данные.

5. Оформление
- В отличии от предыдущих работ в этом я уделил больше внимания на оформление. Разделил проект на 2 основны части: анализ и продакшен. Часть с анализом представляет собой один большой *IPYNB*, а часть с продакшеном состоит из 1 малеьнкого *IPYNB* и прилегающих к ней скриптов.

### Заключение
Последняя часть в списке, но не по значимости оформелние стало для меня новым. И хоть у меня получилось не очень, в будущем я буду стараться улучшать этот навык.